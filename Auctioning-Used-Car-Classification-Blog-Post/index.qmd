---
execute:
  echo: fenced
title: "Auctioning Used-Car Classifier"
toc: true
toc-title: "Table of Contents"
format:
  html:
    embed-resources: true
    code-copy: true
    code-link: true
    code-tools: true
    theme:
      dark: darkly
      light: flatly
  docx: default
  gfm: default
jupyter: python3
---
<!-- title: "Auctioning Used-Car Classifier"
format:
  html:
    code-fold: true
jupyter: python3 -->


#### Author: Andrew Tran


## Blog Post Inspiration and Objectives

In this blog post, I was hoping to take on answering the applying my basic understanding of classification that was taught in this Machine Learning course early in the semseter. Additionally, I was interested in the traditional questions posed by those in the college and just graduating from it as most try to purchase their first car. Specifically, what factors lead to a mutually beneficial first starter car. In the general sense, this dataset does not have a strong corrleation to be applied with Machine Learning algorithms, but I thought I gave it a chance. Through seeing the profit made by selling the car compared to its market-valued price, determining the return-of-investment (ROI) made on the vehicle introduces the concept of binary classification here. With that being said, let's try to tackle this problem with some Machine Learning:

## Data Preprocessing - Cleaning and Analytics

```{python}
# Import needed libraries
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import datetime as dt
import sklearn as sk
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report, precision_recall_fscore_support
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
plt.style.use("fivethirtyeight")
```

First, we will read and display the initial dataset in our file system for this blog post, downloaded from Kaggle. This dataset contains loads of valuable information such as every car's specifications (trim, type of transmission, make, model, color, interior, etc.), state sold, selling price, etc.

```{python}
# Reading and displaying the initial dataset (ignoring any warnings or errors)
df = pd.read_csv("datasets/car_prices.csv", on_bad_lines="skip")
df
```

For clarity on the constraints and parameters of the working datasets, I went to find high-level exploratory statistics on all of the datasets: shape, information about all of the entries, etc.

```{python}
# Determining the shape of the initial dataset
df.shape
```

```{python}
# Getting a sample of the initial dataset through the seeing the first 10 entries
# completely in the dataset
df.head()
```

```{python}
# Figuring out all of the columns (and their names) available for me to use in 
# the dataset
df.columns
```

```{python}
# Getting basic information about the dataset
df.info()
```

Additionally, before handing my combined Book dataset over for Machine Learning training and prediction, I need to clean the data prior to the analysis stage: removing duplicates, deleting null/NaN vales, fixing types of columns, filling invalid values with suitable alternatives, etc.

```{python}
# Figuring out the number of duplicated elements in the dataset (could be 
# problematic if not resolved)
df.duplicated().sum()
```

```{python}
# Renaming the columns to be more readable 
df = df.rename(columns={"sellingprice": "Selling Price", "saledate": "Sale Date"})

cols_rename_dict = {}
for col in df.columns:
    cols_rename_dict.update({col: str(col[0].upper() + col[1:])})

df = df.rename(columns=cols_rename_dict)
df
```

```{python}
# Figuring out the number of 'null'/'NaN' elements in the dataset (i.e. if NaN 
# filling is needed or not)
print(df.isnull().sum())
(df.isnull().sum() / df.shape[0]) * 100
```

```{python}
# Fill unknown and unformatted values with proper ones for readability and to
# improve data accuracy and relevance 
df["Make"].fillna("(Unknown)", inplace=True)
df["Make"].replace("Mercedes-Benz", "M-Benz", inplace=True)
df["Make"].replace("Volkswagen", "VW", inplace=True)
df["Model"].fillna("(Unknown)", inplace=True)
df["Trim"].fillna("(Unknown)", inplace=True)
print(df["Body"].unique(), len(df["Body"].unique()))
df["Body"].fillna(df["Body"].mode()[0], inplace=True)
df["Body"].replace("G Sedan", "gsedan", inplace=True)
df["Body"].replace("g sedan", "gsedan", inplace=True)
df["Body"].replace("Crew Cab", "Crewcab", inplace=True)
df["Body"].replace("crew cab", "crewcab", inplace=True)
df["Transmission"].fillna("Manual", inplace=True)
df["Odometer"].fillna(df["Odometer"].mean(), inplace=True)
df["Condition"].fillna(df["Condition"].mode()[0], inplace=True)
df["Color"].fillna("(Unknown)", inplace=True)
df["Color"].replace("â€”", "(Unknown)", inplace=True)
df["Interior"].fillna(df["Interior"].mode()[0], inplace=True)

df
```

```{python}
# Check to make sure if all NaN and also any unpreferred / unformatted values
# are resolved now
print(df.isnull().sum())
(df.isnull().sum() / df.shape[0]) * 100
```

```{python}
# Convert columns with quantitative data to have a numerical representation 
df["Year"] = df["Year"].apply(int)
df["State"] = df["State"].map(lambda x: x.upper())

# Fix the capitalization on the entries in the columns for readability
def capitalize_first_letter(entry: str):
    if entry == "M-Benz" or entry == "VW" or entry == "BMW":
        return entry
    if entry[0] == "(":
        return entry[:2].upper() + entry[2:].lower()
    else:
        return entry[0].upper() + entry[1:].lower()

df["Make"] = df["Make"].apply(capitalize_first_letter)
df["Model"] = df["Model"].apply(capitalize_first_letter)
df["Body"] = df["Body"].apply(capitalize_first_letter)
df["Transmission"] = df["Transmission"].apply(capitalize_first_letter)
df["Color"] = df["Color"].apply(capitalize_first_letter)
df["Interior"] = df["Interior"].apply(capitalize_first_letter)

def capitalize_first_letter_for_phrase(phrase: str):
    phrase_list: list = phrase.split()
    phrase_list = [capitalize_first_letter(entry) for entry in phrase_list]
    return " ".join(phrase_list)

df["Seller"] = [capitalize_first_letter_for_phrase(entry) for entry in df["Seller"]]

# Convert the "Sales Date" column to become a parsable datetime object
def str_to_datetime(date_str: str):
    month_dict = {"Jan": 1, "Feb": 2, "Mar": 3, "Apr": 4, "May": 5, "Jun": 6, 
            "Jul": 7, "Aug": 8, "Sep": 9, "Oct": 10, "Nov": 11, "Dec": 12}
    time_str_split = tuple(str(date_str).split())
    month, day, year = month_dict[time_str_split[1]], int(time_str_split[2]), int(time_str_split[3])
    smaller_time_str_split = tuple(str(time_str_split[4]).split(":"))
    hours, minutes, seconds = int(smaller_time_str_split[0]), int(smaller_time_str_split[1]), int(smaller_time_str_split[2])    
    return dt.datetime(year, month, day, hours, minutes, seconds)

df["Sale Date"] = df["Sale Date"].apply(str_to_datetime)
df
```

```{python}
# Making the "Vin" column the new index (better identifer/key in dataset)
df.index = df.pop("Vin")
df
```

```{python}
# Rename "Mmr" column to be more readable
df = df.rename(columns={"Mmr": "MMR"})
df
```

In the following 2 code snippets, I tried to filter out the number of entries in the dataset such that to make the data more skewed toward the present, giving us the ability to make conclusions that are relevant to the modern day, but also to avoid issues of utilizing vehicle entries with a (subjectively insignificant) numerical quantity toward future visualizations and future Machine Learning model training.

```{python}
# Drop all entries to only include entries sold between 2005 - 2015
# (making the dataset easier to train Machine Learning models and visualize)
df = df.drop(labels=df[df["Year"] < 2005].index, axis=0)
df
```

```{python}
# Remove car entries that are not significant for reporting in data
# analysis & visualization and to put toward Machine Learning model training
make_counts = df["Make"].value_counts()
df = df[~df["Make"].isin(make_counts[make_counts < 10000].index)]

body_counts = df["Body"].value_counts()
df = df[~df["Body"].isin(body_counts[body_counts < 5000].index)]

color_counts = df["Color"].value_counts()
df = df[~df["Color"].isin(color_counts[color_counts < 5000].index)]

interior_counts = df["Interior"].value_counts()
df = df[~df["Interior"].isin(interior_counts[interior_counts < 1000].index)]

state_counts = df["State"].value_counts()
df = df[~df["State"].isin(state_counts[state_counts < 1000].index)]

df
```

Here, I am trying to offer some visualizations of the cleaned dataset before we pass it over for Machine Learning training and prediction. In this blog post, I wanted to visualize the counts of all of the different types of entries within each descriptive column as a bar graph to show the spread in the graph: Year, Make, Body, Transmission (Type), State (Registered), Color (of Exterior Body), and Interior (Primary Color).

```{python}
# Create bar graphs for descriptive statistics of the cars, figuring out how
# many fall into which group within each qualitative cateogry
def create_bar_graphs(attribute: str):
    sns.countplot(x=attribute, data=df)
    plt.title(f"{attribute} Comparison Across All Auctioning Used-Cars in the Dataset (2005-2015)")
    plt.rcParams["font.size"] = 7
    plt.show()

categorical_columns = ["Year", "Make", "Body", "Transmission", "State", "Color", "Interior"]

for col in categorical_columns:
    create_bar_graphs(col)
```

## Machine Learning - Model Training and Evaluation

Great, now we are onto the Machine Learning part of the blog post!

As seen while cleaning the dataset above, this dataset was more used for data science applications. However, as mentioned above, I found a way to convert this into a Machine-Learning related study. Through seeing the profit made by selling the car compared to its market-valued price, determining the return-of-investment (ROI) made on the vehicle introduces the concept of binary classification here. Thus, to account for the ROI calcuation in my dataframe, I created a new column (as shown below) by subtracting the `Selling Price` columns and the `MMR (Manheim Market Report)` columns.

```{python}
# Convert this into a binary classification problem by separating cars based on
# if each car entry (row) made a profit or not

# First we are calculating the price sold when compared to the average MMR 
# (Manheim Market Report)
df["ROI"] = df["Selling Price"] - df["MMR"]
df
```

As I was curious about the range of the ROI calcuation data, I created a histogram to illustrate the ROI spread across all car models and years currently in my working dataset.

```{python}
# Histogram plot illustrating the ROI across all of the cars
sns.histplot(df["ROI"])
plt.rcParams["font.size"] = 7
plt.title("Histogram ROI Comparison (Against MMR) Across All Auctioning Used-Cars in the Dataset (2005-2015)")
plt.xlim(-8000, 6000)
plt.ylim(0, 10000)
plt.show()
```

Because many of the Machine Learning algoritm functions can only take in numerical, standardized input, I decided to standardized it using True/False (for now), indiciating whether a profit was made or not. Ultimately, it does not matter what the ROI price was for each car but rather if it made a profit (positive ROI) or loss (negative or 0 ROI) at the moment of the sale. Later, I will convert the True/False values to its binary equivalent, 0 and 1.

```{python}
# Converting to the binary classification, slowly reordering the ROI column to 
# binary 0 or 1
# Now: False (failure) and True (Success)
df["ROI"] = ((df["Selling Price"] - df["MMR"]) > 0)
df
```

Once more, before I pass my dataset over to the Machine Learning algorithms, I would like to sort all of the working car entries in my dataframe by all of its important categorical features in a hierarchy for visual reasons using the `sort_values` function - `Sale Date`, `Condition`, `Year`, `Make`, `ROI`, and `Odometer` - so that I can have a better organized picture of the final dataframe I will be working with prior to imposing any Machine Learning onto this.

```{python}
# Sort values in the table in order by column to show a hierarchy of 
# importance between comparable attributes of Auctioning Used-Cars
# from the 2005-2015 time period
df.sort_values(by=["Sale Date", "Condition", "Year", "Make", "ROI", "Odometer"], ascending=[False, False, False, True, False, False], inplace=True)
df
```

With all of the entries sorted into its final positions in the dataframe, I found no need for certain categorical columns to be considered in the Machine Learning algorithms: `Seller`, `MMR`, `Selling Price`, `Trim`, `State`, `Model`, and `Sale Date`. Thus, I dropped them from my working dataframe to consolidate the dataset to those columns that I found relevant for car entry evaluation.

```{python}
# Drop these purely categorical columns (not needed in Machine Learning algorithms
# which require only numerical input)
df.drop(["Seller", "MMR", "Selling Price", 
         "Trim", "State", "Model", "Sale Date"], axis=1, inplace=True)
df
```

As described before, the Machine Learning algoritm functions can only take in numerical, standardized input, so I decided to standardized the all of the rest of the categorical columns used in the evaluation of these used-car entries - `ROI`, `Transmission`, `Exterior Color`, `Matching Colors for Car Exterior and Interior`, `Make`, and `Body` - to numerical inputs using standard index mapping assignment in Python.

```{python}
# Convert all necessary columns to an associated numerical value so that
# they may be accepted as quantiative input to train the Machine Learning model
df = df.rename(columns={"Color": "Exterior Color", "Interior": "Matching Colors for Car Exterior and Interior"})
df["Matching Colors for Car Exterior and Interior"] = (df["Matching Colors for Car Exterior and Interior"] == df["Exterior Color"])

df["ROI"] = df["ROI"].map({True: 1, False: 0}).astype(int)
df["Transmission"] = df["Transmission"].map({"Automatic": 0, "Manual": 1}).astype(int)
df["Matching Colors for Car Exterior and Interior"] = df["Matching Colors for Car Exterior and Interior"].map({True: 1, False: 0}).astype(int)
df["Exterior Color"] = df["Exterior Color"].map({"Black": 0, "White": 1, "Gray": 2, "Silver": 3, "Blue": 4, "Red": 5, "Gold": 6, "Burgundy": 7, "Beige": 8, "Green": 9, "(Unknown)": 10}).astype(int)
df["Make"] = df["Make"].map({"Ford": 0, "Nissan": 1, "Chevrolet": 2, "Toyota": 3, "Dodge": 4, "Honda": 5, "Hyundai": 6, "BMW": 7, "Kia": 8, "Chrysler": 9, "M-Benz": 10, "Jeep": 11, "Infiniti": 12, 
    "VW": 13, "Lexus": 14}).astype(int)
df["Body"] = df["Body"].map({"Sedan": 0, "Suv": 1, "Minivan": 2, "Hatchback": 3, "Coupe": 4, "Crewcab": 5, "Wagon": 6, "Supercrew": 7, "Convertible": 8, "Gsedan": 9}).astype(int)
df
```

As all of the data in the working dataframe are now numerical, I wanted to preliminary-wise see the quantifiable correlation between the `ROI` columns and the rest in the dataset. Thus, using the `corr` function for dataframes with the `ROI` column, I created a heatmap and bar-graph visualizing the constrast between each car entries' attribute columns and its ROI value.

```{python}
# Correlation heatmap to quantify relationships between auctioning used-car
# attributes
plt.figure(figsize=(10, 8))
sns.heatmap(df.corr(), annot=True, linewidths=0.5)
plt.title("Correlation Heatmap Between All Auctioning Used-Cars Quantiative Factors (2005-2015)")
plt.show()

# Correlation bar graph between ROI and all other auctioning used-car
# attributes
target_corr = df.corr()["ROI"].abs().sort_values(ascending=False)
plt.figure(figsize=(10, 8))
sns.barplot(x=target_corr.index[1:], y=target_corr.values[1:])
plt.xticks(rotation=45, ha="right")
plt.xlabel("Auctioned Car Features")
plt.ylabel("Correlation with ROI")
plt.title("Correlation between ROI and Other Auctioned Car Features When Comparing Across All Auctioning Used-Cars in the Dataset (2005-2015)")
plt.tight_layout()
plt.show()
```

Since the dataframe is now properly cleaned, sorted, and integer-mapped by this point, I had split the respective dataframe into the train and test datasets for the Machine Learning model with 80% going to the training dataset and the last 20% going to the test dataset. Fortunately, because order of the data sequentially does not matter here, I was able to utilize the `train_test_split` function for shuffling and randomization, making the future-generated Machine Learning model more unpredictable but also more objective in its returned model results. 

Note that here I also used a `Pipeline` object from the `scikit-learn` package as well as the `MinMaxScaler` and the `StandardScaler` classes. On on hand, the `MinMaxScaler` class is useful for scaling columns to a specific range, usually between [0, 1], to maintain consistency. On the other hand, the `StandardScaler` class is useful for apply Z-score normaization/transformation on the data to avoid sensivite-prone Machine Learning algorithms which require appropriate scaling of the features within its trained dataset. As I learned from online, this `Pipeline` object is necesary to ensure appropriate preprocessing just before the dataset is passed to the Machine Learning model for training and later evaluation. 

```{python}
# Configuring the Machine Learning Tensorflow Model by placing all other columns into the x
# axis and ROI column into the y-axis
X = df.drop(["ROI"], axis=1)
y = df["ROI"]

print("X Shape:", X.shape)
print("Y Shape:", y.shape)

pipeline = Pipeline([
    ("min_max_scaler", MinMaxScaler()),
    ("std_scaler", StandardScaler())
])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=1)
X_train_copy = X_train.copy()
X_train = pipeline.fit_transform(X_train)
X_test = pipeline.transform(X_test)
```

Whenever the Machine Learning model is fitted, it would be a good time to evaluate the effectiveness of my model. Thus, I will calculate metrics (as shown below) such as `Precision Score`, `ROC-AUC (Receiver Operating Characteristic (Curve) - Area Under the Curve) Score`, `Precision Score`, `F1 Score`, `Recall Score`, and the `Confusion Matrix`. Thus, in the following code snippet, I have written a function called `print_score` to print out these statistics as well as a corresponding visualization of the confusion matrix. This function will be invoked after each type of classification model has been discussed to evaluate each one's effectiveness objectively.

```{python}
def print_score(model, X_train, y_train, X_test, y_test, y_train_prob, y_test_prob, train=True):
    if train == True:
        y_pred = model.predict(X_train)
        clf_report = pd.DataFrame(classification_report(y_true=y_train, y_pred=y_pred, output_dict=True, zero_division=0))
        
        # Compute and output statistics for Precision, Accuracy, and ROC AUC Scores as well as the Classifcation Report
        print("Train Result:\n================================================")
        print(f"Accuracy Score: {accuracy_score(y_train, y_pred) * 100:.2f}%")
        print("_______________________________________________")
        print(f"ROC AUC Score: {roc_auc_score(y_train, y_train_prob) * 100:.2f}%")
        print("_______________________________________________")
        print(f"CLASSIFICATION REPORT:\n{clf_report}")
        
        # Calculate and output Precision, Recall, F1-Score, and Confusion Matrix
        precision, recall, f1_score = (0, 0, 0)
        precision, recall, f1_score, _ = precision_recall_fscore_support(y_true=y_train, y_pred=y_pred, average='binary')
        model_performance_metrics: [str] = ["Precision", "Recall", "F1_Score"]
        model_performance_metrics_values: [float] = [precision, recall, f1_score]
        print("_______________________________________________")
        print(f"Precision Score: {precision * 100:.2f}%")
        print("_______________________________________________")
        print(f"Recall Score: {recall * 100:.2f}%")
        print("_______________________________________________")
        print(f"F1 Score: {f1_score * 100:.2f}%")
        print("_______________________________________________")
        print(f"Confusion Matrix:\n{confusion_matrix(y_train, y_pred)}") 

        plt.figure(figsize=(10, 8))
        plt.bar(model_performance_metrics, model_performance_metrics_values, color=["blue", "green", "orange"])
        plt.xlabel("Metrics")
        plt.ylabel("Score")
        plt.title("Recommendation Model Evaluation Metrics -- Training Data")
        plt.show()
        
        plt.figure(figsize=(10, 8))
        sns.heatmap(confusion_matrix(y_train, y_pred), annot=True, fmt='d', cmap='Blues')
        plt.xlabel("Predicted Label")
        plt.ylabel("True Label")
        plt.title("Confusion Matrix for Training Data")
        plt.show()
    
    elif train == False:
        y_pred = model.predict(X_test)
        clf_report = pd.DataFrame(classification_report(y_true=y_test, y_pred=y_pred, output_dict=True, zero_division=0))
        
        # Compute and output statistics for Precision, Accuracy, and ROC AUC Scores as well as the Classifcation Report
        print("Test Result:\n================================================")        
        print(f"Accuracy Score: {accuracy_score(y_test, y_pred) * 100:.2f}%")
        print("_______________________________________________")
        print(f"ROC AUC Score: {roc_auc_score(y_test, y_test_prob) * 100:.2f}%")
        print("_______________________________________________")
        print(f"CLASSIFICATION REPORT:\n{clf_report}")
        
        # Calculate and output Precision, Recall, F1-Score, and Confusion Matrix
        precision, recall, f1_score = (0, 0, 0)
        precision, recall, f1_score, _ = precision_recall_fscore_support(y_true=y_test, y_pred=y_pred, average='binary')
        model_performance_metrics: [str] = ["Precision", "Recall", "F1_Score"]
        model_performance_metrics_values: [float] = [precision, recall, f1_score]
        print("_______________________________________________")
        print(f"Precision Score: {precision * 100:.2f}%")
        print("_______________________________________________")
        print(f"Recall Score: {recall * 100:.2f}%")
        print("_______________________________________________")
        print(f"F1 Score: {f1_score * 100:.2f}%")
        print("_______________________________________________")
        print(f"Confusion Matrix:\n{confusion_matrix(y_test, y_pred)}")  

        plt.figure(figsize=(10, 8))
        plt.bar(model_performance_metrics, model_performance_metrics_values, color=["blue", "green", "orange"])
        plt.xlabel("Metrics")
        plt.ylabel("Score")
        plt.title("Recommendation Model Evaluation Metrics -- Testing Data")
        plt.show() 
        
        plt.figure(figsize=(10, 8))
        sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')
        plt.xlabel("Predicted Label")
        plt.ylabel("True Label")
        plt.title("Confusion Matrix for Testing Data")
        plt.show()
```

Now, we are finally at the Machine Learning classification portion. To reinforce my understanding of the different methodologies covered at the beginning of this course with regards to different ways of doing Machine-Learing classification, I am going to constrast all four of the main classification methods covered in this course: `LogisticRegression`, `KNeighborsClassifier`, `RandomForestClassifier`, and the `DecisionTreeClassifier`, comparing all of them against all of the following metrics evaluated in the `print_score` function to see which method is the most representative and encompassing of the most-accurately depicting my working dataset of Auctioned Used-Cars.

First, I analyzed the `LogisticRegression` Classification algorithm by fitting, estimating the probability of the sample for each class in the model using the `predict_proba` function, and printing out the appropriate metrics and visualization of the confusion matrix using the `print_score` helper function created earlier in the code snippets above.

```{python}
# Utilize the X_train and X_test Datasets to apply it towards LogisticRegression
log_reg = LogisticRegression(random_state=42)
log_reg.fit(X_train, y_train)
y_train_prob = log_reg.predict_proba(X_train)[:, 1]
y_test_prob = log_reg.predict_proba(X_test)[:, 1]

print_score(log_reg, X_train, y_train, X_test, y_test, y_train_prob, y_test_prob, train=True)
print_score(log_reg, X_train, y_train, X_test, y_test, y_train_prob, y_test_prob, train=False)
```

Second, I analyzed the `KNeighborsClassifier` Classification algorithm by fitting, estimating the probability of the sample for each class in the model using the `predict_proba` function, and printing out the appropriate metrics and visualization of the confusion matrix using the `print_score` helper function created earlier in the code snippets above.

```{python}
# Utilize the X_train and X_test Datasets to apply it towards KNeighborsClassifier
knn_classifier = KNeighborsClassifier()
knn_classifier.fit(X_train, y_train)
y_train_prob = knn_classifier.predict_proba(X_train)[:, 1]
y_test_prob = knn_classifier.predict_proba(X_test)[:, 1]

print_score(knn_classifier, X_train, y_train, X_test, y_test, y_train_prob, y_test_prob, train=True)
print_score(knn_classifier, X_train, y_train, X_test, y_test, y_train_prob, y_test_prob, train=False)
```

Third, I analyzed the `RandomForestClassifier` Classification algorithm by fitting, estimating the probability of the sample for each class in the model using the `predict_proba` function, and printing out the appropriate metrics and visualization of the confusion matrix using the `print_score` helper function created earlier in the code snippets above.

```{python}
# Utilize the X_train and X_test Datasets to apply it towards RandomForestClassifier
rf_classifier = RandomForestClassifier(n_estimators= 100, random_state=42)
rf_classifier.fit(X_train, y_train)
y_train_prob = rf_classifier.predict_proba(X_train)[:, 1]
y_test_prob = rf_classifier.predict_proba(X_test)[:, 1]

print_score(rf_classifier, X_train, y_train, X_test, y_test, y_train_prob, y_test_prob, train=True)
print_score(rf_classifier, X_train, y_train, X_test, y_test, y_train_prob, y_test_prob, train=False)
```

For the Random-Forest Classification, I wanted to create an additional visualization seeing what categorical columns were most correlated in predicting the `ROI` column. Thus, I made a bar plot to illustrate and map each categorical relationship's correlation as shown below.

```{python}
# Creates a visualization for the Random-Forest Classifier data
rf_visualization = pd.Series(rf_classifier.feature_importances_, index=df.columns.tolist()[1:]).sort_values(ascending=False)
sns.barplot(x=rf_visualization, y=rf_visualization.index)
plt.xlabel("Feature Importance Score")
plt.yticks(rotation=45, ha="right")
plt.ylabel("Features")
plt.title("Visualizing Important Features of Auctioning Used-Car Data After Random Forest Classification")
plt.show()
```

Lastly, I analyzed the `DecisionTreeClassifier` Classification algorithm by fitting, estimating the probability of the sample for each class in the model using the `predict_proba` function, and printing out the appropriate metrics and visualization of the confusion matrix using the `print_score` helper function created earlier in the code snippets above.

```{python}
# Utilize the X_train and X_test Datasets to apply it towards DecisionTreeClassifier
dt_classifier = DecisionTreeClassifier(max_depth=3, random_state=42)
dt_classifier.fit(X_train, y_train)
y_train_prob = dt_classifier.predict_proba(X_train)[:, 1]
y_test_prob = dt_classifier.predict_proba(X_test)[:, 1]

print_score(dt_classifier, X_train, y_train, X_test, y_test, y_train_prob, y_test_prob, train=True)
print_score(dt_classifier, X_train, y_train, X_test, y_test, y_train_prob, y_test_prob, train=False)
```

For the Random-Forest Classification, I wanted to create an additional visualization seeing what categorical columns were most correlated in predicting the `ROI` column. Thus, I made a bar plot to illustrate and map each categorical relationship's correlation as shown below.

Similar to the Random-Forest Classification, I wanted to create an additional visualization to illustrate a portion of the Decision-Tree using the `plot_tree` function as shown below.

```{python}
# Creates a visualization for the Decision-Tree Classifier data
plt.figure(figsize=(20, 18))
sk.tree.plot_tree(dt_classifier, feature_names=df.columns.tolist()[1:], filled=True, 
                  class_names=["Negative ROI - Loss Money", "Positive ROI - Gain Money / Profited"])
plt.title("Visualizing Decision Tree Classification")
plt.show()
```

## Conclusions

- Based on the four methods of classification invoked here in this blog post, the best classification predictor for this problem out of all of the methods is the `LogisticRegression` Classifier with the highest accuracy score of 64.44% as well as the highest average and consistency of all of the classifier algorithms used here with across the board scoring 61.92%. 

- After reviewing the heatmap called `Correlation Heatmap Between All Aunctioning Used-Cars Quantitative Fcctors (2005-2015)` and the bar graph called `Correlation between ROI and Other Auctioned Car Features When Comparing Across All Auctioning Used-Cars in the Dataset (2005-2015)` in the `Machine Learning - Model Training and Evaluation` section of this blog post, I was surprised to see that the `Condition` qualitative column most correlated with the `ROI` computed column (through the subtraction of the `Selling Price` and the `MMR` (Manheim Market Report) columns) with about approximately `30%` correlation, higher than the 2nd-most correlated column - `Transmission` - by far at approximately `2.5%` correlation. In my opinion, I feel that this could be justified due to the fact that most younger-car buyers prefer a nice, asthetic interior and exterior as a primary selling-point considering to buy a used-car. Most people would take the general assumption that if the used-car to consider buying does not look at initial inspection, most prospective car-buyers would not invest any time further in looking into the performance specifics of the vehicle. This specific pattern could follow when investigating into the `Machine Learning` section that the `RandomForestClassifier` that the `Exterior Color` played a significant factor in this classification algorithm. However, this inference could be ultimately inconclusive due its described randomness.

- With careful experimentation in both the `Data Preprocessing - Cleaning and Analytics` and `Machine Learning - Model Training and Evaluation` sections of this blog post, it is of my opinion that the best factor that impacts `ROI` (and indirectly the classification of Used-Cars by human users) would be the `Condition` of the vehicle in-question at the time of sale.

- Ultimately, I learned a great deal from the blog post experience as I now better understand how to properly utilize different types of simple/basic Machine Learning classification through applying it to a practical, every-day dilemma in our society.

## Reference Sources and Citations (IEEE Format)

To complete this blog post, I used the following online sources as references for developing this:

[1] Car Auctions Dataset (Only Used Dataset Not Kaggle Notebook):

G.S. Deepak Kumar, "Car Auctions - What influences the selling price?", 2021. [Online]. Available: https://www.kaggle.com/code/gsdeepakkumar/car-auctions-what-influences-the-selling-price/input?select=car_prices.csv. [Accessed: 07-Sep.-2023].

[2] Tutorial on Calculating Basic Machine Learning Model Evaluation Statistics:

A. Essam, "Titanic Supervised Learning Classification", Jul.-2023. [Online]. Available: https://www.kaggle.com/code/aliessamali/titanic-supervised-learning-classification. [Accessed: 08-Sep.-2023].

[3] Tutorial on Random Forest Classifiers:

P. Banerjee, "Random Forest Classifer Tutorial", 2019. [Online.] Available: https://www.kaggle.com/code/prashant111/random-forest-classifier-tutorial. [Accessed: 07-Sep.-2023].

