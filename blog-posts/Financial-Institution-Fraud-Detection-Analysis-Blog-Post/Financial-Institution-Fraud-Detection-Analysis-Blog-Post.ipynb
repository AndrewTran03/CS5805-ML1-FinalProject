{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Financial Institution Fraud Detection Analysis\n",
    "\n",
    "#### Author: Andrew Tran"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blog Post Inspiration and Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this blog post, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing - Cleaning and Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imported needed libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, \\\n",
    "    adjusted_rand_score, normalized_mutual_info_score, silhouette_score\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from sklearn.cluster import MiniBatchKMeans, KMeans, DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import kneed\n",
    "plt.style.use(\"fivethirtyeight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading and displaying the initial dataset\n",
    "df = pd.read_csv(\"datasets/bs140513_032310.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining the shape of the initial dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a sample of the initial dataset through the seeing the first 10 entries\n",
    "# completely in the dataset\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figuring out all of the columns (and their names) available for me to use in \n",
    "# the dataset\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting basic information about the dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figuring out the number of duplicated elements in the dataset (could be \n",
    "# problematic if not resolved)\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the columns to be more readable \n",
    "df = df.rename(columns={\"zipcodeOri\": \"ZipCodeOrig\", \"step\": \"TimeStep\"})\n",
    "\n",
    "cols_rename_dict = {}\n",
    "for col in df.columns:\n",
    "    cols_rename_dict.update({col: str(col[0].upper() + col[1:])})\n",
    "\n",
    "df = df.rename(columns=cols_rename_dict)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figuring out the number of 'null'/'NaN' elements in the dataset (i.e. if NaN \n",
    "# filling is needed or not)\n",
    "print(df.isnull().sum())\n",
    "(df.isnull().sum() / df.shape[0]) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the single quotation characters around the following columns' entries\n",
    "single_quotation_cols: [str] = [\"Customer\", \"Age\", \"Gender\", \"ZipCodeOrig\", \"Merchant\", \"ZipMerchant\", \"Category\"]\n",
    "\n",
    "for col in single_quotation_cols:\n",
    "    df[col] = df[col].str.strip(\"'\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Age\"] = df[\"Age\"].map(lambda entry: entry if entry != \"U\" else \"-1\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Category\"] = df[\"Category\"].map(lambda entry: entry.replace(\"es_\", \"\"))\n",
    "\n",
    "df[\"Category\"].replace({\"barsandrestaurants\": \"bars_and_restaurants\", \n",
    "    \"hotelservices\": \"hotel_services\", \"otherservices\": \"other_services\", \n",
    "    \"sportsandtoys\": \"sports_and_toys\", \"wellnessandbeauty\": \"wellness_and_beauty\"}, inplace=True)\n",
    "\n",
    "# Fix the capitalization on the entries in the \"Category\" column for readability\n",
    "def capitalize_first_letter(entry: str):\n",
    "    word_entries = entry.split(\"_\")\n",
    "    word_entries = [(word[0].upper() + word[1:]) for word in word_entries]\n",
    "    return \"_\".join(word_entries)\n",
    "\n",
    "df[\"Category\"] = df[\"Category\"].apply(capitalize_first_letter)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Gender\"] = df[\"Gender\"].map({\"M\": \"Male\", \"F\": \"Female\", \"E\": \"Enterprise\", \"U\": \"Unknown\"})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Age\"] = df[\"Age\"].astype(\"int64\")\n",
    "df[\"ZipCodeOrig\"] = df[\"ZipCodeOrig\"].astype(\"int64\")\n",
    "df[\"ZipMerchant\"] = df[\"ZipMerchant\"].astype(\"int64\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.rcParams[\"font.size\"] = 7\n",
    "sns.barplot(x=df[\"Category\"].unique(), y=df[\"Category\"].value_counts(), palette=sns.color_palette(\"husl\", 8))\n",
    "plt.xlabel(\"Transaction Type\")\n",
    "plt.ylabel(\"Transaction Type Count\")\n",
    "plt.title(\"Different Types of Category Transactions in this Fin. Inst. Fraud Detection Analysis Dataset\")\n",
    "ax = plt.subplot()\n",
    "ax.set_xticklabels(list(df[\"Category\"].unique()),\n",
    "                   rotation=30,\n",
    "                   fontsize=\"8\",\n",
    "                   horizontalalignment=\"right\")\n",
    "plt.show()\n",
    "df[\"Category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.rcParams[\"font.size\"] = 7\n",
    "sns.barplot(x=df[\"Gender\"].unique(), y=df[\"Gender\"].value_counts(), palette=sns.color_palette(\"husl\", 8))\n",
    "plt.xlabel(\"Transaction Type\")\n",
    "plt.ylabel(\"Transaction Type Count\")\n",
    "plt.title(\"Different Types of Entities in this Fin. Inst. Fraud Detection Analysis Dataset\")\n",
    "ax = plt.subplot()\n",
    "ax.set_xticklabels(list(df[\"Gender\"].unique()),\n",
    "                   rotation=30,\n",
    "                   fontsize=\"8\",\n",
    "                   horizontalalignment=\"right\")\n",
    "plt.show()\n",
    "df[\"Gender\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the display precision for floating-point numbers to 3 decimal places\n",
    "pd.set_option(\"display.float_format\", \"{:.2f}\".format)\n",
    "df.loc[:, [\"TimeStep\", \"Amount\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step = Map of unit of time in the real world. 1 step = 1 hour\n",
    "df[\"TimeStep\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.rcParams[\"font.size\"] = 7\n",
    "plt.title('Distribution of Time Feature')\n",
    "sns.countplot(x=\"TimeStep\", data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Amount\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Amount\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount_counts = df[\"Amount\"].value_counts()\n",
    "amount_filter_value: float = 250.00\n",
    "df_amount_dist_display_sample = df[df[\"Amount\"] <= amount_filter_value]\n",
    "num_entries_above_amount_filter_value = df[df[\"Amount\"] > amount_filter_value].shape[0]\n",
    "perc_amount_above_amount_filter_value = (float(num_entries_above_amount_filter_value) / df.shape[0]) * 100\n",
    "print(f\"Note: The number of entries above the filtered amount value of {int(amount_filter_value)} is {num_entries_above_amount_filter_value} \\\n",
    "({perc_amount_above_amount_filter_value:.2f}% of total entries).\")\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.rcParams[\"font.size\"] = 7\n",
    "plt.title('Distribution of Amount Feature')\n",
    "sns.countplot(x=\"Amount\", data=df_amount_dist_display_sample, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_fraud_col = df[\"Fraud\"].value_counts()\n",
    "normal_cases, fraud_cases = counts_fraud_col[0], counts_fraud_col[1]\n",
    "percent_normal = (normal_cases / (normal_cases + fraud_cases)) * 100\n",
    "percent_fraud = (fraud_cases / (normal_cases + fraud_cases)) * 100\n",
    "results = f\"There were {normal_cases} non-fraudulent transactions ({percent_normal:.3f}%) \\\n",
    "and {fraud_cases} fradulent transactions ({percent_fraud:.3f}%)\"\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.rcParams[\"font.size\"] = 7\n",
    "sns.barplot(x=counts_fraud_col.index, y=counts_fraud_col, palette=sns.color_palette(\"husl\", 8))\n",
    "plt.title(\"Comparison of the Number of Fradulent vs. Non-Fraudulent Transactions\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xlabel(\"Class: (0 - Non-Fraudulent vs. 1 - Fradulent)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean Feature Values per Category:\")\n",
    "df_category_grouping_amount_mean = df.groupby('Category')[\"Amount\"].mean()\n",
    "df_category_grouping_fraud_mean = df.groupby('Category')[\"Fraud\"].mean()\n",
    "df_mean = pd.concat([df_category_grouping_amount_mean, df_category_grouping_fraud_mean], keys=[\"Amount\", \"Fraud\"])\n",
    "df_mean = pd.DataFrame(index=df[\"Category\"].unique())\n",
    "df_mean = pd.merge(left=df_mean, right=df_category_grouping_amount_mean, how=\"inner\", left_on=df_mean.index, right_on=df_category_grouping_amount_mean.index)\n",
    "df_mean.rename(columns={\"key_0\": \"Category\"}, inplace=True)\n",
    "df_mean.set_index(keys=\"Category\", drop=True, inplace=True)\n",
    "df_mean = pd.merge(left=df_mean, right=df_category_grouping_fraud_mean, how=\"inner\", left_on=df_mean.index, right_on=df_category_grouping_fraud_mean.index)\n",
    "df_mean.rename(columns={\"key_0\": \"Category\"}, inplace=True)\n",
    "df_mean.set_index(keys=\"Category\", drop=True, inplace=True)\n",
    "df_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_fraud = df[df[\"Fraud\"] == 0]\n",
    "df_fraud = df[df[\"Fraud\"] == 1]\n",
    "\n",
    "pd.concat([df_fraud.groupby(\"Category\")[\"Amount\"].mean(), df_non_fraud.groupby(\"Category\")[\"Amount\"].mean(), \\\n",
    "    df.groupby(\"Category\")[\"Fraud\"].mean() * 100], keys=[\"Fraudulent\", \"Non-Fradulent\", \"Percentage (%)\"], axis=1, \\\n",
    "    sort=False).sort_values(by=[\"Non-Fradulent\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms of the amounts in fraud and non-fraud data\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.rcParams[\"font.size\"] = 7\n",
    "hist_bins: int = 100\n",
    "plt.hist(df_fraud[\"Amount\"], alpha=0.5, label=\"Fraud\", bins=hist_bins)\n",
    "plt.hist(df_non_fraud[\"Amount\"], alpha=0.5, label=\"Non-Fraud\", bins=hist_bins)\n",
    "plt.title(\"Histogram Comparing the Distribution of Fraud vs. Non-Fraud Payments\")\n",
    "plt.xlabel(\"Amount\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xlim(0, 1000)\n",
    "plt.ylim(0, 10000)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(labels=[\"Customer\", \"ZipCodeOrig\", \"Merchant\", \"ZipMerchant\"], axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Amount\"] = df[\"Amount\"].round(2)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Gender\"] = df[\"Gender\"].map({\"Male\": 0, \"Female\": 1, \"Enterprise\": 2, \"Unknown\": 3}).astype(\"int64\")\n",
    "df[\"Category\"] = df[\"Category\"].map({\"Transportation\": 0, \"Food\": 1, \"Health\": 2, \"Wellness_And_Beauty\": 3,\n",
    "                                    \"Fashion\": 4, \"Bars_And_Restaurants\": 5, \"Hyper\": 6, \"Sports_And_Toys\": 7,\n",
    "                                    \"Tech\": 8, \"Home\": 9, \"Hotel_Services\": 10, \"Other_Services\": 11, \n",
    "                                    \"Contents\": 12, \"Travel\": 13, \"Leisure\": 14}).astype(\"int64\")\n",
    "display(df.info())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap to quantify relationships between auctioning used-car\n",
    "# attributes\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.rcParams[\"font.size\"] = 7\n",
    "sns.heatmap(df.corr(), annot=True, linewidths=0.5)\n",
    "plt.title(\"Correlation Heatmap Between All Financial Institution Quantiative Factors (2005-2015)\")\n",
    "plt.show()\n",
    "\n",
    "# Correlation bar graph between ROI and all other auctioning used-car\n",
    "# attributes\n",
    "target_corr = df.corr()[\"Fraud\"].abs().sort_values(ascending=False)\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.rcParams[\"font.size\"] = 7\n",
    "sns.barplot(x=target_corr.index[1:], y=target_corr.values[1:], palette=sns.color_palette(\"husl\", 8))\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.xlabel(\"Auctioned Car Features\")\n",
    "plt.ylabel(\"Correlation with Fraud\")\n",
    "plt.title(\"Correlation between Fraud and Other Features When Comparing Across All Financial Transactions in the Dataset (2005-2015)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning - Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we are onto the Machine Learning part of the blog post!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop([\"Fraud\"], axis=1)\n",
    "y = df[\"Fraud\"]\n",
    "\n",
    "print(\"X Shape:\", X.shape)\n",
    "print(\"Y Shape:\", y.shape)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"std_scaler\", StandardScaler()),\n",
    "    (\"min_max_scaler\", MinMaxScaler())\n",
    "])\n",
    "\n",
    "X_scaled = pipeline.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, shuffle=True, random_state=1)\n",
    "X_train = pipeline.fit_transform(X_train)\n",
    "X_test = pipeline.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_model = KMeans()\n",
    "kmeans_elbow_visualizer = KElbowVisualizer(kmeans_model, k=(1, 11))\n",
    "kmeans_elbow_visualizer.fit(X_scaled)\n",
    "kmeans_elbow_visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the kmeans model on scaled data\n",
    "kmeans_model = KMeans(n_clusters=4, random_state=42).fit(X_train)\n",
    "\n",
    "# Get the cluster number for each datapoint\n",
    "X_test_clusters = kmeans_model.predict(X_test)\n",
    "\n",
    "# Save the cluster centroids\n",
    "X_test_clusters_centers = kmeans_model.cluster_centers_\n",
    "\n",
    "# Obtain predictions and calculate distance from cluster centroid (using Euclidean distance)\n",
    "kmeans_dist = [np.linalg.norm(x - y) for x, y in zip(X_test, X_test_clusters_centers[X_test_clusters])]\n",
    "\n",
    "y_pred = np.array(kmeans_dist)\n",
    "y_pred[kmeans_dist >= np.percentile(kmeans_dist, 95)] = 1\n",
    "y_pred[kmeans_dist >= np.percentile(kmeans_dist, 95)] = 0\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_report = pd.DataFrame(classification_report(y_true=y_test, y_pred=y_pred, output_dict=True, zero_division=0))\n",
    "conf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_true=y_test, y_pred=y_pred) * 100:.2f}%\")\n",
    "print(\"_______________________________________________\")\n",
    "print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n",
    "print(\"_______________________________________________\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.rcParams[\"font.size\"] = 7\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_min_samples = 2 * (X.shape[1])\n",
    "dbscan_min_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = NearestNeighbors(n_neighbors=dbscan_min_samples)\n",
    "nn_model_fit = nn_model.fit(X_scaled)\n",
    "nn_distances, nn_indices = nn_model_fit.kneighbors(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_distances = np.sort(nn_distances, axis=0)\n",
    "nn_distances = nn_distances[:, 1]\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.rcParams[\"font.size\"] = 7\n",
    "plt.plot(nn_distances)\n",
    "plt.xlabel(\"Data Points\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.title(\"Distances Between Neighbors for All Data Points\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the inflection point (x, y) using the Kneed package\n",
    "kneedle_model = kneed.KneeLocator(y=nn_distances, x=np.arange(0, X.shape[0]), \n",
    "                                  S=1.0, curve=\"convex\", direction=\"increasing\")\n",
    "nn_model_inflection_point = [kneedle_model.knee, kneedle_model.knee_y]\n",
    "nn_model_inflection_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the kneedle model\n",
    "kneedle_model.plot_knee()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Fraud\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_nn_model_inflection_point_y_coord = round(nn_model_inflection_point[1], 2)\n",
    "rounded_nn_model_inflection_point_y_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dbscan_eps in np.arange(0.005, round(rounded_nn_model_inflection_point_y_coord, 2) + 0.005, 0.005):\n",
    "    print()\n",
    "    print(f\"            ------------------            \")\n",
    "    print(f\"---------- | For eps = {round(dbscan_eps, 3):.3f}: | ----------\")\n",
    "    print(f\"            ------------------            \")\n",
    "    # Creating the model:\n",
    "    dbscan= DBSCAN(eps=dbscan_eps, min_samples=dbscan_min_samples).fit(X_scaled)\n",
    "    # Integrating the classification made by DBSCAN to the original dataset:\n",
    "    df[\"dbscan_labels\"] = dbscan.labels_\n",
    "    # Checking the number of points attributed to each cluster and the number of outliers ('dbscan_labels' == -1) identified bt the model:\n",
    "    print(df[\"dbscan_labels\"].value_counts())\n",
    "    # Checking to which cluster(s) the model added most cases of fraud ('Fraud' == 1):\n",
    "    anomalies = df.loc[df[\"Fraud\"] == 1]\n",
    "    print(anomalies[\"dbscan_labels\"].value_counts())\n",
    "    print(f\"Number of Clustered Outliers: {anomalies['dbscan_labels'].value_counts()[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_model = DBSCAN(eps=nn_model_inflection_point[1], min_samples=dbscan_min_samples)\n",
    "dbscan_model.fit(X_train)\n",
    "\n",
    "train_cluster_labels = dbscan_model.labels_\n",
    "test_cluster_labels = dbscan_model.fit_predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the ARI for the training and testing data\n",
    "ari_dbscan_train = adjusted_rand_score(y_train, train_cluster_labels)\n",
    "ari_dbscan_test = adjusted_rand_score(y_test, test_cluster_labels)\n",
    "\n",
    "# Calculate the NMI for the training and testing data\n",
    "nmi_dbscan_train = normalized_mutual_info_score(y_train, train_cluster_labels)\n",
    "nmi_dbscan_test = normalized_mutual_info_score(y_test, test_cluster_labels)\n",
    "\n",
    "#  Calculate the Silhouette score for the training and testing data \n",
    "silhouette_train = silhouette_score(X_train, train_cluster_labels)\n",
    "silhouette_test = silhouette_score(X_test, test_cluster_labels)\n",
    "\n",
    "print(f\"ARI for Training Data: {ari_dbscan_train}\")\n",
    "print(f\"ARI for Testing Data: {ari_dbscan_test}\")\n",
    "print(f\"NMI for Training Data: {nmi_dbscan_train}\")\n",
    "print(f\"NMI for Testing Data: {nmi_dbscan_test}\")\n",
    "print(f\"Silhouette Score for Training Data: {silhouette_train}\")\n",
    "print(f\"Silhouette Score for Testing Data: {silhouette_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize outputs of DBSCAN\n",
    "color_labels = dbscan_model.labels_\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.rcParams[\"font.size\"] = 7\n",
    "plt.title(\"DBSCAN Performance Visualzation\")\n",
    "plt.scatter(X, y, c=color_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and display outliers dataframe from DBSCAN\n",
    "dbscan_outliers = pd.DataFrame(df[dbscan_model.labels_ == -1])\n",
    "dbscan_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO-DO\n",
    "\n",
    "- Heatmap\n",
    "- Drop all needed columns\n",
    "- train_test_split\n",
    "- Figure out which ML algorithm to use\n",
    "- Do DBSCAN (as recommended)\n",
    "- Report classification statistics if needed\n",
    "- DONE..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "\n",
    "- Data: https://www.kaggle.com/datasets/ealaxi/paysim1/data (NOT USED)\n",
    "\n",
    "- Data: https://www.kaggle.com/datasets/ealaxi/banksim1 (USED)\n",
    "\n",
    "- KMeans Elbow: https://www.kaggle.com/code/javigallego/outliers-eda-clustering-tutorial\n",
    "\n",
    "- KMeans: https://www.kaggle.com/code/mohamedisbaine/fraud-detection\n",
    "\n",
    "- NearestNeighbor, KNeedle, DBSCAN Reference: https://www.kaggle.com/code/rodmnzs/fraud-detection-clustering-with-dbscan\n",
    "\n",
    "- DBSCAN Reference #2: https://medium.com/@dilip.voleti/dbscan-algorithm-for-fraud-detection-outlier-detection-in-a-data-set-60a10ad06ea8\n",
    "\n",
    "- Seaborne Color Palette: https://seaborn.pydata.org/tutorial/color_palettes.html\n",
    "\n",
    "- https://www.kaggle.com/code/mukulkirti/outlier-or-anomalies-detection-and-removal#3.3-DBScane-Anomaly-Detection\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
